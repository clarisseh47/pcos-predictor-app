{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WDDBnx0O528"
      },
      "source": [
        "# Setting Up the Notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R5Tl0xqcerB"
      },
      "outputs": [],
      "source": [
        "# Access Kaggle's API to get the path to the dataset.\n",
        "\n",
        "!pip install kaggle\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ankushpanday1/pcos-prediction-datasettop-75-countries\")\n",
        "filepath = path + '/pcos_prediction_dataset.csv'\n",
        "\n",
        "print(\"Path to dataset files:\", filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcIwF6r7PEBB"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll be using: pandas, matplotlib, sklearn\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E55zIRiPQ5I"
      },
      "source": [
        "# Technovation ML Component: PCOS Risk Predictor\n",
        "\n",
        "### We followed [the machine learning process](https://www.codecademy.com/article/the-ml-process) to train and evaluate a model that predicts PCOS risk.\n",
        "\n",
        "**The Machine Learning Process:**\n",
        "\n",
        "\n",
        "1.   Formulate a Question\n",
        "2.   Find and Understand the Data\n",
        "3.   Clean the Data and Feature Engineer\n",
        "4.   Choose a Model\n",
        "5.   Tune and Evaluate\n",
        "6.   Use the Model and Present Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaJp9BZSQH-o"
      },
      "source": [
        " **What do we want to find out? What will we predict?**\n",
        "\n",
        " First, we need to learn more about our topic. What tool can we build that addresses a problem around our topic?\n",
        "\n",
        "### Our Questions:\n",
        "\n",
        "1. What is PCOS?\n",
        "2. What are risk factors and symptoms associated with PCOS?\n",
        "3. Can we predict if someone is at high or low risk for having PCOS?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfNuwhvCRB93"
      },
      "source": [
        "## Collect data and visualize the data.\n",
        "\n",
        "We've found a dataset on Kaggle that we'll explore.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioa36S82SFJ7"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "pcos_df = pd.read_csv(filepath)\n",
        "\n",
        "# Preview the dataframe.\n",
        "pcos_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExwutUrKSp76"
      },
      "outputs": [],
      "source": [
        "# Print out a summary of the dataframe to check for missing values.\n",
        "pcos_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktUYCs9xTDPP"
      },
      "outputs": [],
      "source": [
        "# Look into the column with missing/null values.\n",
        "pcos_df['Acne Severity'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzcICkmQJoi0"
      },
      "outputs": [],
      "source": [
        "# Fill in the NaN values with \"No acne\", instead of None\n",
        "pcos_df['Acne Severity'].fillna(value='No Acne',inplace=True)\n",
        "pcos_df['Acne Severity'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kfnEbS4Nr4-"
      },
      "outputs": [],
      "source": [
        "# Make plots to look visually for trends.\n",
        "severity_counts = pcos_df['Acne Severity'].value_counts()\n",
        "plt.bar(severity_counts.index,severity_counts)\n",
        "print(severity_counts)\n",
        "plt.title('Counts of Acne Severity')\n",
        "plt.xlabel('Acne Severity')\n",
        "plt.ylabel('Counts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18wiIYPflxzm"
      },
      "outputs": [],
      "source": [
        "# Look at each column's data type and decide which columns need to be encoded.\n",
        "pcos_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttrn731Msk6O"
      },
      "outputs": [],
      "source": [
        "# Import the OrdinalEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Create a list of columns that we'll encode using the ordinal encoder\n",
        "columns_to_encode = ['BMI','Menstrual Regularity','Hirsutism','Acne Severity','Family History of PCOS','Insulin Resistance','Stress Levels','Urban/Rural','Socioeconomic Status','Fertility Concerns','Ethnicity']\n",
        "# Create a list of columns that we'll store the encoded columns in (ex. \"BMI encoded\" for \"BMI\" column)\n",
        "encoded_columns = ['BMI encoded','Menstrual Regularity encoded','Hirsutism encoded','Acne Severity encoded','Family History of PCOS encoded','Insulin Resistance encoded','Stress Levels encoded','Urban/Rural encoded','Socioeconomic Status encoded','Fertility Concerns encoded','Ethnicity encoded']\n",
        "\n",
        "# Create an instance of OrdinalEncoder. Then, store the unique variables of each column in columns_to_encode in ORDER in a list\n",
        "BMI_category = ['Underweight','Normal','Overweight','Obese']\n",
        "MR_category = ['Regular','Irregular']\n",
        "Hirsutism_category = ['No','Yes']\n",
        "AS_category = ['No acne','Mild','Moderate','Severe']\n",
        "FM_of_PCOS_category = ['No','Yes']\n",
        "IR_category = ['No','Yes']\n",
        "SL_category = ['Low','Medium','High']\n",
        "UR_category = ['Urban','Rural']\n",
        "SS_category = ['Low','Middle','High']\n",
        "FC_category = ['No','Yes']\n",
        "E_category = ['African','Asian','Caucasian','Hispanic','Other']\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[BMI_category,MR_category,Hirsutism_category,AS_category,FM_of_PCOS_category,IR_category,SL_category,UR_category,SS_category,FC_category,E_category])\n",
        "\n",
        "# Fit and transform the data\n",
        "pcos_df[encoded_columns] = encoder.fit_transform(pcos_df[columns_to_encode])\n",
        "\n",
        "# Check that we've added the columns we want to the df.\n",
        "pcos_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4yL2upRl3Yj"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe with only the relevant columns.\n",
        "knn_columns = encoded_columns\n",
        "knn_columns.append('Age')\n",
        "knn_columns.append('Lifestyle Score')\n",
        "print(knn_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGklcfKbl6qj"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing datasets.\n",
        "x_train,x_test,y_train,y_test = train_test_split(pcos_df[knn_columns],pcos_df['Diagnosis'],test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMpEuQSmZeLM"
      },
      "outputs": [],
      "source": [
        "# Print out the number of rows in the training set\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydW9myPoZepY"
      },
      "outputs": [],
      "source": [
        "# Create and fit model\n",
        "model = KNeighborsClassifier(n_neighbors=310)\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(x_train)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XVtCrmJf7BA"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_train,predictions)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_train,predictions)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_train,predictions)\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6LpLHNLQUaC"
      },
      "outputs": [],
      "source": [
        "# Create and fit weighted KNN model\n",
        "weighted_model = KNeighborsClassifier(n_neighbors=310, weights=\"distance\")\n",
        "weighted_model.fit(x_train,y_train)\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = weighted_model.predict(x_train)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_train,predictions)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_train,predictions)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_train,predictions)\n",
        "print(classification_rep)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = weighted_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_test,predictions)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_test,predictions)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_test,predictions)\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9-AdkHqQVV7"
      },
      "outputs": [],
      "source": [
        "# SMOTE = Synthetic Minority Oversampling Technique\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Apply SMOTE to oversample the minority class\n",
        "smote = SMOTE(random_state=42)\n",
        "x_train_resampled, y_train_resampled = smote.fit_resample(x_train,y_train)\n",
        "\n",
        "\n",
        "# How many rows are in the resampled training set?\n",
        "print(x_train_resampled.shape)\n",
        "print(y_train_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b1MBEVmQXWE"
      },
      "outputs": [],
      "source": [
        "# Train KNN on the resampled data\n",
        "smote_model = KNeighborsClassifier(n_neighbors=415, weights=\"distance\")\n",
        "smote_model.fit(x_train_resampled,y_train_resampled)\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = smote_model.predict(x_train)\n",
        "\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_train,predictions)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_train,predictions)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_train,predictions)\n",
        "print(classification_rep)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = smote_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_test,predictions)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_test,predictions)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_test,predictions)\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKdJ4yQ6QZrB"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning with grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the model without any hyperparameters\n",
        "grid_search_knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "params = {\n",
        "    'n_neighbors': [265,315,365,415,465,515,565],\n",
        "    'weights': ['uniform','distance']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV and fit it to the training data\n",
        "grid_search_smote = GridSearchCV(estimator=grid_search_knn\n",
        ", param_grid=params)\n",
        "grid_search_smote.fit(x_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best hyperparameters (hint: use the best_params_ attribute)\n",
        "best_params = grid_search_smote.best_params_\n",
        "best_model = grid_search_smote.best_estimator_\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Use the best model from grid search to make predictions on the test set\n",
        "y_pred = best_model.predict(x_test)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"accuracy\")\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "print(accuracy)\n",
        "print(\"confusion matrix\")\n",
        "confusion_mat = confusion_matrix(y_test,y_pred)\n",
        "print(confusion_mat)\n",
        "print(\"classification report\")\n",
        "classification_rep = classification_report(y_test,y_pred)\n",
        "print(classification_rep)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to Google Drive:\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the file to your Google Drive\n",
        "with open('/content/drive/MyDrive/model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)"
      ],
      "metadata": {
        "id": "1TgkgFYUlVQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}